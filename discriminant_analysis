import pickle
import numpy as np
import pandas as pd
import sklearn as sl
import matplotlib.pyplot as plt
import xlsxwriter as xlsxw

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, roc_auc_score


##########
def process_analysis(new_data, method:str='l', plot:bool=False):
    X_train, X_test, y_train, y_test = new_data
    X = np.concatenate((X_train, X_test), axis=0)
    y = np.concatenate((y_train, y_test), axis=0)
    
    if method=='l':
        analyser = linear_discriminant_analyser(X_train, y_train)
        trans = dimensionality_reduction(analyser, X)
        print(trans.shape)
        if plot:
            plot_reduction(trans, y)
    elif method=='q':
        analyser = quadratic_discriminant_analyser(X_train, y_train)
    elif method=='f':
        analyser = forest_classifier(X_train, y_train)
    elif method=='r':
        analyser = LogisticRegression()
        analyser.fit(X_train, y_train)
    y_pred = analyser.predict(X_test)

    if plot:
        probs = analyser.predict_proba(X_test)
        probs = probs[:, 1]  
        fper, tper, thresholds = roc_curve(y_test, probs) 
        # print('fper:', fper<=0.1)
        # print('tper:', tper>=0.9)
        print('thresholds:', np.min(thresholds[fper<=0.1]))
        print('thresholds:', np.max(thresholds[tper>=0.9]))
        plot_roc_curve(fper, tper)
    
    eval = evaluate_confusion_matrix(y_train, analyser.predict(X_train))
    eval = evaluate_confusion_matrix(y_test, y_pred)
    return eval


###
def load_data():
    with open('mypicklefile', 'rb') as f1:
        d = pickle.load(f1)
    return d

def print_shapes(k, X_train, X_test, y_train, y_test):
    print()
    print(k, ": ")
    print('x_train:', X_train.shape, 'y_train:', y_train.shape)
    print('x_test:', X_test.shape, 'y_test:', y_test.shape)
    return


##########
def quadratic_discriminant_analyser(X_train, y_train):
    qda = QDA()
    qda.fit(X_train, y_train)
    return qda

def forest_classifier(X_train, y_train):
    classifier = RandomForestClassifier(max_depth=2, random_state=0)
    classifier.fit(X_train, y_train)
    return classifier

def linear_discriminant_analyser(X_train, y_train):
    lda = LDA(n_components=None, store_covariance=True)
    lda.fit(X_train, y_train)
    print(lda.covariance_.shape)
    print(lda.coef_.shape)
    # coef_to_excel(lda.covariance_)
    return lda

def dimensionality_reduction(analyser, X):
    var_ratios = analyser.explained_variance_ratio_
    trans = analyser.transform(X)
    return trans

###
def plot_reduction(trans, y):
    grave = y == 1
    lMax = trans.shape[0]
    N = np.count_nonzero(grave)
    print(trans[grave].shape)
    print(trans[~grave].shape)
    plt.close('all')
    plt.scatter(trans[grave], np.zeros(N), marker='+', color='r', label='forme grave')
    plt.scatter(trans[~grave], np.zeros(lMax-N), marker='+', color='b', label='forme légère')
    plt.legend()
    plt.show()
    return

def select_n_components(var_ratio, goal_var: float) -> int:
    total_variance = 0.0
    n_components = 0
    for explained_variance in var_ratio:
        total_variance += explained_variance
        n_components += 1
        if total_variance >= goal_var:
            break
    return n_components

def plot_roc_curve(fper, tper):  
    plt.plot(fper, tper, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

###
def evaluate_confusion_matrix(y_test, y_pred):
    cm = confusion_matrix(y_test, y_pred)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    print(tn, fp, fn, tp)
    se = tp / (tp+fn)
    sp = tn / (tn+fp)
    auc = roc_auc_score(y_test, y_pred)

    print()
    print(cm)
    print('Accuracy: %.4f %%' % (acc*100))
    print('F1 Score: %.4f %%' % (f1*100))
    print('Sensitivity: %.4f %%' % (se*100))
    print('Specificity: %.4f %%' % (sp*100))
    print('Area Under Curve: %.4f %%' % (auc*100))
    return ([acc, f1, se, sp, auc])


##########
if __name__ == "__main__":
    from separate_data import load_data
    new_data = load_data()
    eval = process_analysis(new_data, method='l', plot=True)
    print()
    print(eval)


###
def eval_mean_n(N):
    evals = []
    for k in range(N):
        eval = process_analysis(data, method='l', shuff=True)
        evals.append(eval)
    eval_mean = np.array(evals).sum(axis=0)
    print()
    print("mean: ", eval_mean)
    return eval_mean

def coef_to_excel(tab): 
    # workbook = xlsxw.Workbook('coefs.xlsx') 
    # worksheet = workbook.add_worksheet("My sheet") 
    # row0, col0 = 1, 1
    # i_max, j_max = tab.shape
    # for i in range(i_max):
    #     for j in range(j_max):
    #         worksheet.write(row0 + i, col0 + j, tab[i, j])
    # workbook.close()

    df = pd.DataFrame(tab, index = col_names, columns = col_names)
    df.to_excel (r'D:\-CHARLES-\VSCodeProjects\SleeveProject\data\coefs.xlsx', index = False, header=True)
    df.style.background_gradient(cmap='coolwarm').set_precision(2)
    return