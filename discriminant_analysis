import numpy as np
import pandas as pd
import sklearn as sl
import matplotlib.pyplot as plt
import xlsxwriter as xlsxw

from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import ElasticNet, LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, roc_auc_score, classification_report


##########
def process_analysis(new_data, method:str='lda', plot:bool=False, grid:bool=False):
    X_train, X_test, y_train, y_test = new_data
    X = np.concatenate((X_train, X_test), axis=0)
    y = np.concatenate((y_train, y_test), axis=0)

    if method=='lda':
        analyser = linear_discriminant_analyser(X_train, y_train)
    elif method=='qda':
        analyser = quadratic_discriminant_analyser(X_train, y_train)
    elif method=='rf':
        analyser = forest_classifier(X_train, y_train)
    elif method=='lr':
        analyser = logistic_regression_analyser(X_train, y_train)
    elif method=='gb':
        analyser = gradient_boosting_classifier(X_train, y_train, X_test, y_test)
    y_pred = analyser.predict(X_test)
    # coefs_to_excel(analyser.coef_)

    if grid:
        k_fold_acc = cross_val_score(analyser, X_train, y_train, cv=5)
        k_fold_mean = k_fold_acc.mean()
        print('\n', 'k_fold_acc:', k_fold_acc)
        print('k_fold_mean:', k_fold_mean)
        # print("Best Parameters:", analyser.best_params_)
        # print("Train Score:", analyser.best_score_)
        print("Test Score:", analyser.score(X_test, y_test))

    if plot:
        probs = analyser.predict_proba(X_test)
        probs = probs[:, 1]  
        fper, tper, thresholds = roc_curve(y_test, probs) 
        # print('fper:', fper<=0.1)
        # print('tper:', tper>=0.9)
        th_max = np.min(thresholds[fper<=0.1])
        th_min = np.max(thresholds[tper>=0.9])
        print('\n', 'threshold:', th_min)
        print('threshold:', th_max)
        plot_probs(y_test, probs, [th_min, th_max])
        plot_roc_curve(fper, tper)
    
    eval = evaluate_confusion_matrix(y_train, analyser.predict(X_train))
    eval = evaluate_confusion_matrix(y_test, y_pred)
    return eval

def model_selection(analyser, X_train, y_train):
    k_fold_acc = cross_val_score(analyser, X_train, y_train, cv=5)
    k_fold_mean = k_fold_acc.mean()
    print('\n', 'k_fold_acc:', k_fold_acc)
    print('k_fold_mean:', k_fold_mean)

    analyser = LogisticRegression(penalty=ElasticNet)
    return

###
def print_shapes(k, X_train, X_test, y_train, y_test):
    print('\n', k, ": ")
    print('x_train:', X_train.shape, 'y_train:', y_train.shape)
    print('x_test:', X_test.shape, 'y_test:', y_test.shape)
    return


##########
def quadratic_discriminant_analyser(X_train, y_train):
    qda = QuadraticDiscriminantAnalysis()
    qda.fit(X_train, y_train)
    return qda

def forest_classifier(X_train, y_train):
    rf = RandomForestClassifier(max_depth=2, random_state=0)
    rf.fit(X_train, y_train)
    return rf

def linear_discriminant_analyser(X_train, y_train):
    lda = LinearDiscriminantAnalysis(n_components=None, store_covariance=True)
    lda.fit(X_train, y_train)
    return lda

def logistic_regression_analyser(X_train, y_train):
    lr = LogisticRegression()
    lr.fit(X_train, y_train)
    return lr

def gradient_boosting_classifier(X_train, y_train, X_test, y_test):
    grid = {
    'learning_rate':[0.01,0.05,0.1],
    'n_estimators':np.arange(100,500,100),
    # 'max_depth':[2,3,4,5,6,7]
    }
    gb = GradientBoostingClassifier(learning_rate=0.01, n_estimators=200)
    gb.fit(X_train, y_train)
    # gb_cv = GridSearchCV(gb, grid, cv=5)
    # gb_cv.fit(X_train, y_train)
    # print("Best Parameters:", gb_cv.best_params_)
    # print("Train Score:", gb_cv.best_score_)
    # print("Test Score:", gb_cv.score(X_test, y_test))
    return gb

###
def plot_probs(y_test, probs, xcoords):
    grave = y_test == 1
    lMax = probs.shape[0]
    N = np.count_nonzero(grave)
    # print(probs[grave].shape)
    # print(probs[~grave].shape)
    plt.close('all')
    for xc in xcoords:
        plt.axvline(x=xc, color='c', linestyle='--')
    plt.scatter(probs[grave], np.zeros(N), marker='+', color='r', label='forme grave')
    plt.scatter(probs[~grave], np.zeros(lMax-N), marker='+', color='b', label='forme légère')
    plt.legend()
    plt.show()
    return

def select_n_components(var_ratio, goal_var: float) -> int:
    total_variance = 0.0
    n_components = 0
    for explained_variance in var_ratio:
        total_variance += explained_variance
        n_components += 1
        if total_variance >= goal_var:
            break
    return n_components

def plot_roc_curve(fper, tper):  
    plt.plot(fper, tper, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

###
def evaluate_confusion_matrix(y_test, y_pred):
    cm = confusion_matrix(y_test, y_pred)
    acc = accuracy_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    se = tp / (tp+fn)
    sp = tn / (tn+fp)
    f1 = 2* (acc*se) / (acc+se)
    # f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred)

    print('\n', cm)
    print('Accuracy: %.2f %%' % (acc*100))
    print('Sensitivity: %.2f %%' % (se*100))
    print('Specificity: %.2f %%' % (sp*100))
    print('F1 Score: %.2f %%' % (f1*100))
    print('Area Under Curve: %.2f %%' % (auc*100))
    # print('\n', classification_report(y_test, y_pred))
    return ([acc, f1, se, sp, auc])

###
def eval_mean_n(N):
    evals = []
    for k in range(N):
        eval = process_analysis(new_data, method='rf', shuff=True)
        evals.append(eval)
    eval_mean = np.array(evals).sum(axis=0)
    print('\n', "mean: ", eval_mean)
    return eval_mean

def coefs_to_excel(tab):
    from separate_data import get_col_names
    # workbook = xlsxw.Workbook('coefs.xlsx') 
    # worksheet = workbook.add_worksheet("My sheet") 
    # row0, col0 = 1, 1
    # i_max, j_max = tab.shape
    # for i in range(i_max):
    #     for j in range(j_max):
    #         worksheet.write(row0 + i, col0 + j, tab[i, j])
    # workbook.close()
    col_names = get_col_names()[:-1]
    print('\n', tab)
    print(col_names)
    df = pd.DataFrame(tab, columns = col_names)
    df.style.background_gradient(cmap='coolwarm').set_precision(2).to_excel(r'D:\-CHARLES-\VSCodeProjects\SleeveProject\data\coefs.xlsx', index = False, header=True)
    return


##########
if __name__ == "__main__":
    from separate_data import load_data, get_col_names
    new_data = load_data()
    eval = process_analysis(new_data, method='gb', plot=True, grid=True)